{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10550808,"sourceType":"datasetVersion","datasetId":6528110}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import librosa\nimport torch\nimport types\nimport torch.nn as nn\nfrom transformers import AutoModelForAudioClassification\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (Wav2Vec2Model,\n                                                  Wav2Vec2PreTrainedModel)\n\n\nsignal = torch.from_numpy(\n    librosa.load('/kaggle/input/speech-dataset/UrbanSound8K/audio/fold3/102105-3-0-0.wav', sr=16000)[0])[None, :]\ndevice = 'cpu'\n\nclass ADV(nn.Module):\n\n    def __init__(self, config):\n\n        super().__init__()\n\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, x):\n\n        x = self.dense(x)\n        x = torch.tanh(x)\n\n        return self.out_proj(x)\n\n\nclass Dawn(Wav2Vec2PreTrainedModel):\n  \n\n    def __init__(self, config):\n\n        super().__init__(config)\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = ADV(config)\n\n    def forward(self, x):\n        x -= x.mean(1, keepdim=True)\n        variance = (x * x).mean(1, keepdim=True) + 1e-7\n        x = self.wav2vec2(x / variance.sqrt())\n        return self.classifier(x.last_hidden_state.mean(1))\n\n\ndef _forward(self, x):\n    '''x: (batch, audio-samples-16KHz)'''\n    x = (x + self.config.mean) / self.config.std  # sgn\n    x = self.ssl_model(x, attention_mask=None).last_hidden_state\n    # pool\n    h = self.pool_model.sap_linear(x).tanh()\n    w = torch.matmul(h, self.pool_model.attention).softmax(1)\n    mu = (x * w).sum(1)\n    x = torch.cat(\n        [\n            mu,\n            ((x * x * w).sum(1) - mu * mu).clamp(min=1e-7).sqrt()\n        ], 1)\n    return self.ser_model(x)\n\n\n# WavLM\n\nbase = AutoModelForAudioClassification.from_pretrained(\n        '3loi/SER-Odyssey-Baseline-WavLM-Multi-Attributes',\n        trust_remote_code=True).to(device).eval()\nbase.forward = types.MethodType(_forward, base)\n\n# Wav2Vec2\n\ndawn = Dawn.from_pretrained(\n    'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n).to(device).eval()\n\n\ndef wav2small(x):\n    return .5 * dawn(x) + .5 * base(x)\n\npred = wav2small(signal.to(device))\nprint(f'Arousal={pred[0, 0]} '\n      f'Dominance={pred[0, 1]} ',\n      f'Valence={pred[0, 2]}')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T16:51:11.796371Z","iopub.execute_input":"2025-02-02T16:51:11.796728Z","iopub.status.idle":"2025-02-02T16:52:10.176323Z","shell.execute_reply.started":"2025-02-02T16:51:11.796700Z","shell.execute_reply":"2025-02-02T16:52:10.174752Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67724983d70440aab457146a1994cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pipeline_utils.py:   0%|          | 0.00/5.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87bd972f5c5146c6a1028e9402ae455c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/3loi/SER-Odyssey-Baseline-WavLM-Multi-Attributes:\n- pipeline_utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84965394e4c448a08c378fae7f47d71e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5530e526fd624031a3a63aded1bfd702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02c165128164f45af5f3d465a8ddf2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ede96cf0e5645b39816b6925e8822b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/661M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79af9a0960054006b05a77d299ab99bd"}},"metadata":{}},{"name":"stdout","text":"Arousal=0.7052952647209167 Dominance=0.5967236757278442  Valence=0.5533943772315979\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}